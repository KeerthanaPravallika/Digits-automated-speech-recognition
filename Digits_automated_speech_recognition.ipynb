{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Digits automated speech recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVvMMeachkO59DkrNU9bCg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeerthanaPravallika/Digits-automated-speech-recognition/blob/main/Digits_automated_speech_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset link : https://github.com/Jakobovski/free-spoken-digit-dataset"
      ],
      "metadata": {
        "id": "oQQbSECURl2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OcIkJ8HQtHk"
      },
      "outputs": [],
      "source": [
        "#include the dataset \n",
        "\n",
        "#copying the zip file link and use wget command\n",
        "\n",
        "!wget https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/heads/master.zip \n",
        "\n",
        "#unzip the dataset folder\n",
        "! unzip master.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the data"
      ],
      "metadata": {
        "id": "JxJrmhSTecVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spectrogram :** Spectrogram is a visual representation of the spectrum of frequencies of a signal."
      ],
      "metadata": {
        "id": "e1h8RgBYS9j7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting the audio clips to spectrograms**"
      ],
      "metadata": {
        "id": "mz7v2bseWGp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary modules for converting into spectrograms\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import scipy.io.wavfile as wav\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "metadata": {
        "id": "08-WK0XHRCVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters:\n",
        "  # Path of audio file\n",
        "  # Destination folder path\n",
        "  # Spectrogram dimenensions\n",
        "  # Number of overlap\n",
        "  # colour scheme\n",
        "\n",
        "def convert_wav_to_spectrogram(audio_path, dest_path, spectrogram_dim=(64, 64), noverlap=16, cmap='gray_r'):\n",
        "  sample_rate, samples = wav.read(audio_path) #reading the audio file\n",
        "\n",
        "  # setting its size in inches\n",
        "  fig = plt.figure()\n",
        "  fig.set_size_inches((spectrogram_dim[0]/fig.get_dpi(), spectrogram_dim[1]/fig.get_dpi()))\n",
        "\n",
        "  # set the axis of the figure\n",
        "  ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
        "  ax.set_axis_off()\n",
        "  fig.add_axes(ax)\n",
        "\n",
        "  # spectrogram creation\n",
        "  ax.specgram(samples, cmap=cmap, Fs=2, noverlap=noverlap)\n",
        "\n",
        "  # set the locator for the x and y axis\n",
        "  ax.xaxis.set_major_locator(plt.NullLocator())\n",
        "  ax.yaxis.set_major_locator(plt.NullLocator())\n",
        "\n",
        "  # Saving  the figure in the destination path\n",
        "  fig.savefig(dest_path, bbox_inches=\"tight\", pad_inches=0)\n"
      ],
      "metadata": {
        "id": "mt2ahvqWWgTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterating over recordings folder and converts its contents\n",
        "\n",
        "\n",
        "def dir_to_spectrogram(audio_dir, spectrogram_dir, spectrogram_dimensions=(64, 64), noverlap=16, cmap='gray_r'):\n",
        "  file_names = [f for f in listdir(audio_dir) if isfile(join(audio_dir, f)) and '.wav' in f]\n",
        "  for file_name in file_names:\n",
        "    #print(file_name)\n",
        "    audio_path = audio_dir + file_name\n",
        "    spectogram_path = spectrogram_dir + file_name.replace('.wav', '.png')\n",
        "    convert_wav_to_spectrogram(audio_path, spectogram_path, spectrogram_dim=spectrogram_dimensions, noverlap=noverlap, cmap=cmap)"
      ],
      "metadata": {
        "id": "U4B_vYvaZPqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_files_folder = \"recordings/\"\n",
        "spectrogram_folder = \"Spectrograms/\"\n",
        "dir_to_spectrogram(audio_files_folder, spectrogram_folder)"
      ],
      "metadata": {
        "id": "wfT4D1jQZ9FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "metadata": {
        "id": "KLxPLLy-aCon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spliting the name of the file\n",
        "# Ex : 0_george_4\n",
        "# 0 - Digit , georger - performer, 4 - sample number\n",
        "\n",
        "imagesDir = \"Spectrograms/\"\n",
        "trainset = []\n",
        "testset = []\n",
        "for file in os.listdir(imagesDir):\n",
        "  label = file.split('_')[0]\n",
        "  sample_number = file.split('_')[2]\n",
        "  img = image.load_img(imagesDir+file)\n",
        "\n",
        "  # Dividing into train and testing dataset\n",
        "  if sample_number in ['0.png','1.png','2.png','3.png','4.png']:\n",
        "    testset.append([image.img_to_array(img), label])\n",
        "  else:\n",
        "    trainset.append([image.img_to_array(img), label])"
      ],
      "metadata": {
        "id": "JnBIuCoWb44l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividing into featues and target \n",
        "\n",
        "# Getting only images in the train list\n",
        "X_train = [item[0] for item in trainset]\n",
        "# Getting only Labels in the train list\n",
        "y_train = [item[1] for item in trainset]\n",
        "\n",
        "\n",
        "# Getting only images in the test list \n",
        "X_test = [item[0] for item in testset]\n",
        "# Getting only Labels in the test list \n",
        "y_test = [item[1] for item in testset]"
      ],
      "metadata": {
        "id": "VapGH55qdE56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert list to numpy array \n",
        "\n",
        "X_train = np.asanyarray(X_train)\n",
        "y_train = np.asanyarray(y_train)\n",
        "X_test = np.asanyarray(X_test)\n",
        "y_test = np.asanyarray(y_test)"
      ],
      "metadata": {
        "id": "DstkOVmydbkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to one hot representation\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Normalize the images\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "metadata": {
        "id": "j1H1xgMceQ4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Designing"
      ],
      "metadata": {
        "id": "EI9LCHgBesRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization # for creating CNN\n",
        "from keras import models\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "1f-Xoq7aekXr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3]) # setting the shape to (64,64,1)\n",
        "\n",
        "def basic_cnn():\n",
        "  model = Sequential()\n",
        "\n",
        "  # three layers of convolution\n",
        "  # batch normalization will decrease the image size\n",
        "  model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=data_shape))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  # max pooling layer for extracting the useful features in the image \n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # a dropout layer will discard some neurons in the network and that will help reduce overfitting in the model\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # adding a fully connected dense layer of 128 neurons with radioactivation along with a batch normalization and another dropout layer\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  # adding another dense layer of 64 neurons with radioactivation along with a batch normalization and another dropout layer\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  #  output dense layer is of 10 neurons because we need to recognize 10 digits from 0 to 9 \n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  \n",
        "  #model.compile(optimizer='Adadelta',loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  metrics=['accuracy'])\n",
        "  model.compile(loss = 'categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "HHoV9bNCe4TX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_model = basic_cnn()\n",
        "first_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AluCuV2xg5_9",
        "outputId": "8834c5a8-95b1-4dd7-d7d1-9d1557448843"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_18 (Conv2D)          (None, 63, 63, 32)        416       \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (None, 63, 63, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 62, 62, 48)        6192      \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (None, 62, 62, 48)       192       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 61, 61, 120)       23160     \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 61, 61, 120)      480       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 30, 30, 120)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 30, 30, 120)       0         \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 108000)            0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 128)               13824128  \n",
            "                                                                 \n",
            " batch_normalization_33 (Bat  (None, 128)              512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,864,370\n",
            "Trainable params: 13,863,586\n",
            "Non-trainable params: 784\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_model.fit(X_train, y_train, batch_size = 50, validation_split=0.2, epochs = 20, verbose = 1)"
      ],
      "metadata": {
        "id": "PddOYpR5mWvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for accuracy\n",
        "\n",
        "first_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "GdpgnGw-hK3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model\n",
        "first_model.save(\"spoken_digit_recognition_.h5\")"
      ],
      "metadata": {
        "id": "QVYhRlntkiAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 33\n",
        "print('ground Truth',np.argmax(y_test[index]))\n",
        "print('Prediction' ,np.argmax(first_model.predict(X_test[index].reshape(1,64,64,3))))"
      ],
      "metadata": {
        "id": "Qu0bDEOQmCdd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}